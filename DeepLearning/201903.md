# Image Classification
1. Gradient-Based Learning Applied to Document Recognition. (LeNet-5, 1998, Yann LeCun)
2. AlexNet: Imagenet classification with deep convolutional neural networks. (NIPS 2012, Krizhevsky Alex)
3. Network in Network. (2014, Min Lin, National University of Singapore)
4. VGGNet: Very Deep Convolutional Networks for Large-Scale Image Recognition. (ICLR 2015, Simonyan)
5. GoogLeNet:Going deeper with convolution.(CVPR 2015, Christian Szegedy, Google)
6. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.(2015, Sergey Ioffe, Google)
7. Rethinking the Inception Architecture for Computer Vision. (2015, Christian Szegedy, Google)
8. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.(2016, Christian Szegedy, Google)
9. Deep Residual Learning for Image Recognition. (2015, Kaiming He, Microsoft)
10. Identity Mappings in Deep Residual Networks. (2016, Kaiming He, Microsoft)
11. Wide Residual Networks. (2017, Sergey Zagoruyko)
12. DenseNet: Densely Connected Convolutional Networks. (2016, Gao Huang, Cornell University)
13. ResNeXt: Aggregated Residual Transformations for Deep Neural Networks.(2017, Saining Xie, UCSD && Facebook)
14. SENet: Squeeze-and-Excitation Networks.(2018,Jie Hu, MomentaAI)

# Object Detection
1. R-CNN
2. Fast R-CNN
3. Faster R-CNN
4. Scalable Object Detection using Deep Neural Networks. (2013, Dumitru Erhan, Google)
5. SPPNet: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. (2015, Kaiming He, )
6. SSD: Single Shot MultiBox Detector. (2016, WeiLiu, UNC Chapel Hill)
7. R-FCN: Object Detection via Region-based Fully Convolutional Networks. (2016, Jifeng Dai, Microsoft)
8. You Only Look Once: Unified, Real-Time Object Detection. (Joseph Redmon, University of Washington)
9. YOLO9000: Better, Faster, Stronger. (Joseph Redmon, University of Washington)
10. YOLOv3: An Incremental Improvement. (Joseph Redmon, University of Washington)
11. Feature Pyramid Networks for Object Detection. (2017, Tsung-Yi Lin, Facebook)
12. Light-Head R-CNN: In Defense of Two-Stage Object Detector. (2017, Zeming Li, Tsinghua && megvii)
13. Quantization Mimic: Towards Very Tiny CNN for Object Detection. (2018, Yi Wei, Tsinghua && sensetime)
14. RetinaNet: Focal Loss for Dense Object Detection. (2018, Tsung-Yi Lin, Facebook)

# Sematic Segmatation
1. Fully Convolutional Networks for Semantic Segmentation. (CVPR 2015 Jonathan Long)
2. SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation. (Vijay Badrinarayanan University of Cambridge)
3. Multi-scale Context Aggregation by Dilated Convolutions. (ICLR 2016 Fisher Yu Princeton University && Intel)
4. Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs. (ICLR 2015 Liang-Chieh Chen UCLA && Google)
5. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. (APRIL 2018 Liang-Chieh Chen)
6. Rethinking Atrous Convolution for Semantic Image Segmentation. (Liang-Chieh Chen)

# Model Compression
## Review
1. Model compression as constrained optimization, with application to neural nets. Part I: general framework
2. Model compression as constrained optimization, with application to neural nets. Part II: quantization.
3. A Survey of Model Compression and Acceleration for Deep Neural Networks
## Low Rank Approximation && Tensor Decomposition
1. Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation. (2014, Emily Denton, New York University && Facebook)
2. Efficient and Accurate Approximations of Nonlinear Convolutional Networks. (2014, Xiangyu Zhang, Xi’an Jiaotong University && Microsoft)
3. Accelerating Very Deep Convolutional Networks for Classification and Detection. (2015, Xiangyu Zhang)
4. Convolutional Neural Networks With Low-rank Regularization. (ICLR 2016, Cheng Tai, Princeton University)
5. Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications. (ICLR 2016, Yong-Deok Kim, Samsung)
 
## Network Pruning && Sparsity Constraint
1. Learning bothWeights and Connections for Efficient Neural Networks. (2015, Song Han, Stanford University)
2. Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization And Huffman Coding. (ICLR 2016, Song Han, Stanford University)
3. Dynamic Network Surgery for Efficient DNNs. (2016, Yiwen Guo, Intel, [Code](https://github.com/yiwenguo/Dynamic-Network-Surgery))
4. Learning to Prune: Exploring the Frontier of Fast and Accurate Parsing. (2016, Tim Vieira, Johns Hopkins University)
5. Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning. (2017, Tien-Ju Yang, MIT)
6. Faster CNNs with Direct Sparse Convolutions and Guided Pruning. (ICLR 2017, Jongsoo Park, Intel)
7. Fine-Pruning: Joint Fine-Tuning and Compression of a Convolutional Network with Bayesian Optimization. (2017, Frederick Tung)
8. Pruning Filters for Efficient Convnets. (ICLR 2017, Hao Li, University of Maryland && NEC Lab)
9. Pruning Convolutional Neural Networks for Resource Efficient Inference. (ICLR 2017, Pavlo Molchanov, NVIDIA)
10 Soft Weight-sharing for Neural Network Compression (ICLR 2017, Karen Ullrich, University of Amsterdam)
11. Channel Pruning for Accelerating Very Deep Neural Networks. (2017, Yihui He, Xi’an Jiaotong University && Megvii)
12. Data-Driven Sparse Structure Selection for Deep Neural Networks. (2017, Zehao Huang, TuSimple)
13. Accelerating Convolutional Networks via Global & Dynamic Filter Pruning. (IJCAI-18, Shaohui Lin, Xiamen University && Tencent)

## Parameter Quantization && Low Precision Approximation
1. Compressing Deep Convolutional Networkd Using Vector Quantization. (ICLR 2015, Yunchao Gong, Facebook)
2. Binarized Neural Networks: Training Neural Networks withWeights and Activations Constrained to +1 or -1. (2016, Matthieu Courbariaux, Universit´e de Montr´eal)
3. Effective Quantization Methods for Recurrent Neural Networks. (2016, Qinyao He, Megvii)
4. Local Binary Convolutional Neural Networks. (2016, Felix Juefei-Xu, CMU)
5. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. (2016, Mohammad Rastegari, Allen Institute for AI && University of Washington)
6. Quantized Convolutional Neural Networks for Mobile Devices. (2016, Jiaxiang Wu, Chinese Academy of Sciences)
7. Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations. (2016, Itay Hubara, Israel Institute of Technology)
8. Binarized Convolutional Neural Networks with Separable Filters for Efficient Hardware Acceleration. (2017, Jeng-Hau Lin, UCSD && UCLA)
9. Quantization Mimic: Towards Very Tiny CNN for Object Detection. (2018, Yi Wei1y, Tsinghua && Sensetime)
10. Dorefa-Net: Training Low Bitwidth Convolutional Neural Networks With Low Bitwidth Gradients. (2018, Shuchang Zhou, Megvii)
11. Improving the speed of neural networks on CPUs. (Vincent Vanhoucke, Google)

## Knowledge Distillation
